{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2-i8jBl9GRH"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redislabs-Solution-Architects/financial-vss/blob/main/redisvl-02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# RAG from scratch with RedisVL (Python)\n",
        "\n",
        "![Redis](https://redis.com/wp-content/themes/wpx/assets/images/logo-redis.svg?auto=webp&quality=85,75&width=120)\n",
        "\n",
        "This notebook uses [redisvl](https://redisvl.com), a dedicated Python client library for using Redis as a vector database, to perform document + embdding indexing, semantic search, and RAG with an LLM (from scratch)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT9HzsnQ1uiz"
      },
      "source": [
        "## Setup and Data Prep\n",
        "\n",
        "### Pull Github Materials\n",
        "We need to clone the supporting materials from github."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJJ2UW6M1ui0"
      },
      "outputs": [],
      "source": [
        "# This clones your git repository into a directory named 'temp_repo'.\n",
        "!git clone https://github.com/Redislabs-Solution-Architects/financial-vss.git temp_repo\n",
        "\n",
        "# This command moves the 'resources' directory from 'temp_repo' to your current directory.\n",
        "!mv temp_repo/resources .\n",
        "!mv temp_repo/requirements.txt .\n",
        "\n",
        "# This deletes the 'temp_repo' directory, cleaning up the unwanted files.\n",
        "!rm -rf temp_repo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z67mf6T91ui2"
      },
      "source": [
        "### Install Python Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgxBQFXQ1ui2"
      },
      "outputs": [],
      "source": [
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTeBg4AvjdYs"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrtWWU4I1ui3"
      },
      "source": [
        "### Load and Extract PDF Docs\n",
        "\n",
        "Now we will load a single financial (10k filings) doc and preprocess it using some LangChain helpers.\n",
        "\n",
        "\n",
        "- `UnstructuredFileLoader` is not the only document loader type that LangChain provides. Docs: https://python.langchain.com/docs/integrations/document_loaders/unstructured_file\n",
        "- `RecursiveCharacterTextSplitter` is what we use to create smaller chunks of text from the doc. Docs: https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uijl2qFH1ui3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Load list of pdfs\n",
        "data_path = \"resources/\"\n",
        "docs = [os.path.join(data_path, file) for file in os.listdir(data_path)]\n",
        "\n",
        "print(\"Listing available documents ...\", docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anya8hVnT6K_"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "\n",
        "# pick out the Nike doc for this exercise\n",
        "doc = [doc for doc in docs if \"nke\" in doc][0]\n",
        "\n",
        "loader = UnstructuredFileLoader(\n",
        "    doc, mode=\"single\", strategy=\"fast\"\n",
        "  )\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2500, chunk_overlap=0\n",
        "  )\n",
        "\n",
        "chunks = loader.load_and_split(text_splitter)\n",
        "\n",
        "print(\"Done preprocessing. Created\", len(chunks), \"chunks of the original pdf\", doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di35riAn9o3s"
      },
      "source": [
        "### Create dense propositions from raw text\n",
        "\n",
        "One technique we can use to improve the quality of retrieval is to leverage an LLM from OpenAI during ETL. We will prompt the LLM to summarize and decompose the raw pdf text into more discrete propositional phrases. This will enhance the clarity of the text and improve semantic retrieval for RAG.\n",
        "\n",
        "The goal is to utilize a preprocessing technique similar to what's outlined here:\n",
        "https://github.com/langchain-ai/langchain/blob/master/templates/propositional-retrieval/propositional_retrieval/proposal_chain.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ9khrZu5jYV"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "\n",
        "CHAT_MODEL = \"gpt-3.5-turbo-0125\"\n",
        "\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg-W0XIM5tsw"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import json\n",
        "\n",
        "\n",
        "def create_dense_props(chunk):\n",
        "    \"\"\"Create dense representation of raw text content.\"\"\"\n",
        "    # The system message here should be HEAVILY customized for your specific use case\n",
        "    SYSTEM_PROMPT = \"\"\"\n",
        "    You are a helpful PDF extractor tool. You will be presented with segments from\n",
        "    raw PDF documents composed of 10k SEC filings information about public companies.\n",
        "\n",
        "    Decompose and summarize the raw content into clear and simple propositions,\n",
        "    ensuring they are interpretable out of context. Consider the following rules:\n",
        "    1. Split compound sentences into simpler dense phrases that retain existing\n",
        "    meaning.\n",
        "    2. Simplify technical jargon or wording if possible while retaining existing\n",
        "    meaning.\n",
        "    2. For any named entity that is accompanied by additional descriptive information,\n",
        "    separate this information into its own distinct proposition.\n",
        "    3. Decontextualize the proposition by adding necessary modifier to nouns or\n",
        "    entire sentences and replacing pronouns (e.g., \"it\", \"he\", \"she\", \"they\", \"this\", \"that\")\n",
        "    with the full name of the entities they refer to.\n",
        "    4. Present the results as a list of strings, formatted in JSON, under the key \"propositions\".\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.OpenAI().chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        response_format={ \"type\": \"json_object\" },\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": f\"Decompose this raw content using the rules above:\\n{chunk.page_content} \"}\n",
        "        ]\n",
        "    )\n",
        "    res = response.choices[0].message.content\n",
        "    return json.loads(res)[\"propositions\"]\n",
        "\n",
        "\n",
        "\n",
        "props = [\n",
        "    create_dense_props(chunk) for chunk in tqdm.tqdm(chunks)\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMtZa1vP8GZS"
      },
      "outputs": [],
      "source": [
        "# Compose chunks and generated propositions\n",
        "print(\"Raw\\n\", chunks[8].page_content)\n",
        "print(\"\\nCleaned\\n\", props[8])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBjCgE-J1ui6"
      },
      "source": [
        "### Create text embeddings from \"propositions\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH_tyNbf1ui9"
      },
      "outputs": [],
      "source": [
        "from redisvl.utils.vectorize import HFTextVectorizer\n",
        "\n",
        "hf = HFTextVectorizer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Embed each set of propositions derived by the LLM\n",
        "embeddings = hf.embed_many([\" \".join(p) for p in props])\n",
        "\n",
        "# Check to make sure we've created enough embeddings, 1 per document chunk\n",
        "len(embeddings) == len(props) == len(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p3jimVo1ui9"
      },
      "source": [
        "### Run Localized Redis Stack\n",
        "\n",
        "If you don't have a remote Redis instance, use an in-notebook version of [Redis Stack](https://redis.io/docs/getting-started/install-stack/). Or you can provision your own free instance of [Redis Cloud](https://redis.com/try-free/).\n",
        "\n",
        "\n",
        "Use the below code to download and run a localized version of Redis Stack here in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3CTLZza1ui9"
      },
      "outputs": [],
      "source": [
        "%%sh\n",
        "curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\n",
        "echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list\n",
        "sudo apt-get update  > /dev/null 2>&1\n",
        "sudo apt-get install redis-stack-server  > /dev/null 2>&1\n",
        "redis-stack-server --daemonize yes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBIY8Vi1ui-"
      },
      "source": [
        "### Connect to Redis\n",
        "\n",
        "By default this notebook would connect to the local instance of Redis Stack. If you have your own Redis Cloud instance - replace REDIS_PASSWORD, REDIS_HOST and REDIS_PORT values with your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENQXsci01ui-"
      },
      "outputs": [],
      "source": [
        "# Replace values below with your own if using Redis Cloud instance\n",
        "REDIS_HOST = os.getenv(\"REDIS_HOST\", \"localhost\") # \"redis-18374.c253.us-central1-1.gce.cloud.redislabs.com\"\n",
        "REDIS_PORT = os.getenv(\"REDIS_PORT\", \"6379\")      # 18374\n",
        "REDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\", \"\")  # \"1TNxTEdYRDgIDKM2gDfasupCADXXXX\"\n",
        "\n",
        "# Construct URL\n",
        "REDIS_URL = f\"redis://:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5baI0xDQ1ui-"
      },
      "source": [
        "## Getting Started with RedisVL\n",
        "\n",
        "### Create an index from schema\n",
        "Below we connect to Redis and create an index for vector search that contains a single text field and vector field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zB1EW_9n1ui-"
      },
      "outputs": [],
      "source": [
        "from redis import Redis\n",
        "from redisvl.schema import IndexSchema\n",
        "from redisvl.index import SearchIndex\n",
        "\n",
        "index_name = \"redisvl\"\n",
        "\n",
        "schema = IndexSchema.from_dict({\n",
        "  \"index\": {\n",
        "    \"name\": index_name,\n",
        "    \"prefix\": \"chunk\"\n",
        "  },\n",
        "  \"fields\": [\n",
        "    {\n",
        "        \"name\": \"content\",\n",
        "        \"type\": \"text\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"propositions\",\n",
        "        \"type\": \"text\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"label\",\n",
        "        \"type\": \"tag\",\n",
        "        \"attrs\": {\n",
        "            \"sortable\": True\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"text_embedding\",\n",
        "        \"type\": \"vector\",\n",
        "        \"attrs\": {\n",
        "            \"dims\": hf.dims,\n",
        "            \"distance_metric\": \"cosine\",\n",
        "            \"algorithm\": \"hnsw\",\n",
        "            \"datatype\": \"float32\"\n",
        "        }\n",
        "    }\n",
        "  ]\n",
        "})\n",
        "\n",
        "# connect to redis\n",
        "client = Redis.from_url(REDIS_URL)\n",
        "\n",
        "# create an index\n",
        "index = SearchIndex(schema, client)\n",
        "index.create(overwrite=True, drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6GOqmeN1ui_"
      },
      "outputs": [],
      "source": [
        "# use the CLI to see the created index\n",
        "!rvl index listall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C70C-UWj1ujA"
      },
      "outputs": [],
      "source": [
        "!rvl index info -i redisvl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qrj-jeGmBRTL"
      },
      "source": [
        "### Process and load data using RedisVL\n",
        "Below we use the RedisVL index to simply load the list of document chunks to Redis db."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zsg09Keg1ujA"
      },
      "outputs": [],
      "source": [
        "# load expects an iterable of dictionaries\n",
        "from redisvl.redis.utils import array_to_buffer\n",
        "\n",
        "data = [\n",
        "    {\n",
        "        'label': f'ID-{i}',\n",
        "        'content': chunk.page_content,\n",
        "        'propositions': \" \".join(props[i]),\n",
        "        # For HASH -- must convert embeddings to bytes\n",
        "        'text_embedding': array_to_buffer(embeddings[i])\n",
        "    } for i, chunk in enumerate(chunks)\n",
        "]\n",
        "\n",
        "# RedisVL handles batching automatically\n",
        "keys = index.load(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZsFB-6Z1ujB"
      },
      "source": [
        "### Query the database\n",
        "Now we can use the RedisVL index to perform similarity search operations with Redis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkFv-_iC1ujB"
      },
      "outputs": [],
      "source": [
        "from redisvl.query import VectorQuery\n",
        "\n",
        "query = \"Nike profit margins and company performance\"\n",
        "\n",
        "query_embedding = hf.embed(query)\n",
        "\n",
        "vector_query = VectorQuery(\n",
        "    vector=query_embedding,\n",
        "    vector_field_name=\"text_embedding\",\n",
        "    num_results=3,\n",
        "    return_fields=[\"label\", \"propositions\"],\n",
        "    return_score=True\n",
        ")\n",
        "\n",
        "# show the raw redis query\n",
        "str(vector_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5reL5qTW1ujC"
      },
      "outputs": [],
      "source": [
        "# execute the query with RedisVL\n",
        "index.query(vector_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZrcd6n7T6LE"
      },
      "outputs": [],
      "source": [
        "# paginate through results\n",
        "for result in index.paginate(vector_query, page_size=1):\n",
        "    print(result[0][\"label\"], result[0][\"vector_distance\"], flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ap6WqPLT6LE"
      },
      "source": [
        "### Sort by alternative fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daLVm6OkLn9T"
      },
      "outputs": [],
      "source": [
        "# Sort by label field after vector search limits to topK\n",
        "vector_query = VectorQuery(\n",
        "    vector=query_embedding,\n",
        "    vector_field_name=\"text_embedding\",\n",
        "    num_results=4,\n",
        "    return_fields=[\"label\"],\n",
        "    return_score=True\n",
        ")\n",
        "\n",
        "# Decompose vector_query into the core query and the params\n",
        "query = vector_query.query\n",
        "params = vector_query.params\n",
        "\n",
        "# Pass query and params direct to index.search()\n",
        "result = index.search(\n",
        "    query.sort_by(\"label\", asc=True),\n",
        "    params\n",
        "  )\n",
        "\n",
        "[doc.__dict__ for doc in result.docs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81PoXomtT6LF"
      },
      "source": [
        "### Add filters to vector queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a11G3xXJ1ujC"
      },
      "outputs": [],
      "source": [
        "from redisvl.query.filter import Text\n",
        "\n",
        "vector_query = VectorQuery(\n",
        "    vector=query_embedding,\n",
        "    vector_field_name=\"text_embedding\",\n",
        "    num_results=4,\n",
        "    return_fields=[\"propositions\"],\n",
        "    return_score=True\n",
        ")\n",
        "\n",
        "# Set a text filter\n",
        "text_filter = Text(\"content\") % \"profit\"\n",
        "\n",
        "vector_query.set_filter(text_filter)\n",
        "\n",
        "index.query(vector_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XvVv8zAT6LF"
      },
      "source": [
        "### Range queries in RedisVL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCffoZRx1ujD"
      },
      "outputs": [],
      "source": [
        "from redisvl.query import RangeQuery\n",
        "\n",
        "range_query = RangeQuery(\n",
        "    vector=query_embedding,\n",
        "    vector_field_name=\"text_embedding\",\n",
        "    num_results=4,\n",
        "    return_fields=[\"propositions\"],\n",
        "    return_score=True,\n",
        "    distance_threshold=0.5  # find all items with a semantic distance of less than 0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gHmam1Q1ujD"
      },
      "outputs": [],
      "source": [
        "index.query(range_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZg4U21r1ujD"
      },
      "outputs": [],
      "source": [
        "# Add filter to range query\n",
        "range_query.set_filter(text_filter)\n",
        "\n",
        "index.query(range_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYYPTQN7T6LG"
      },
      "source": [
        "## Building a RAG Pipeline with RedisVL\n",
        "\n",
        "We're going to build a complete RAG pipeline from scratch incorporating the following components:\n",
        "\n",
        "- Standard retrieval and chat completion\n",
        "- Query re-writing to improve accuracy\n",
        "- Semantic caching to improve performance\n",
        "- Conversational session history to improve personalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCWlVR2OT6LG"
      },
      "source": [
        "### Setup RedisVL AsyncSearchIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_esLGYzbT6LG"
      },
      "outputs": [],
      "source": [
        "from redis.asyncio import Redis\n",
        "from redisvl.index import AsyncSearchIndex\n",
        "\n",
        "client = Redis.from_url(REDIS_URL)\n",
        "index = AsyncSearchIndex(index.schema, client)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8Af-zneT6LH"
      },
      "source": [
        "### Define OpenAI RAG Helpers & Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V1Tio4-ZjmA"
      },
      "outputs": [],
      "source": [
        "\n",
        "async def answer_question(index: AsyncSearchIndex, query: str):\n",
        "    \"\"\"Answer the user's question\"\"\"\n",
        "\n",
        "    SYSTEM_PROMPT = \"\"\"You are a helpful financial analyst assistant that has access\n",
        "    to public financial 10k documents in order to answer users questions about company\n",
        "    performance, ethics, characteristics, and core information.\n",
        "    \"\"\"\n",
        "\n",
        "    query_vector = hf.embed(query)\n",
        "    # Fetch context from Redis using vector search\n",
        "    context = await retrieve_context(index, query_vector)\n",
        "    # Generate contextualized prompt and feed to OpenAI\n",
        "    response = await openai.AsyncClient().chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": promptify(query, context)}\n",
        "        ],\n",
        "        temperature=0.1,\n",
        "        seed=42\n",
        "    )\n",
        "    # Response provided by LLM\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "async def retrieve_context(index: AsyncSearchIndex, query_vector) -> str:\n",
        "    \"\"\"Fetch the relevant context from Redis using vector search\"\"\"\n",
        "    results = await index.query(\n",
        "        VectorQuery(\n",
        "            vector=query_vector,\n",
        "            vector_field_name=\"text_embedding\",\n",
        "            return_fields=[\"propositions\"],\n",
        "            num_results=3\n",
        "        )\n",
        "    )\n",
        "    content = \"\\n\".join([result[\"propositions\"] for result in results])\n",
        "    return content\n",
        "\n",
        "\n",
        "def promptify(query: str, context: str) -> str:\n",
        "    return f'''Use the provided context below derived from public financial\n",
        "    documents to answer the user's question. If you can't answer the user's\n",
        "    question, based on the context; do not guess. If there is no context at all,\n",
        "    respond with \"I don't know\".\n",
        "\n",
        "    User question:\n",
        "\n",
        "    {query}\n",
        "\n",
        "    Helpful context:\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Answer:\n",
        "    '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgVM_g01T6LP"
      },
      "source": [
        "### Vanilla Async RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn-PoACdbihY"
      },
      "outputs": [],
      "source": [
        "# Generate a list of questions\n",
        "questions = [\n",
        "    \"What is the trend in the company's revenue and profit over the past few years?\",\n",
        "    \"What are the company's primary revenue sources?\",\n",
        "    \"How much debt does the company have, and what are its capital expenditure plans?\",\n",
        "    \"What does the company say about its environmental, social, and governance (ESG) practices?\",\n",
        "    \"What is the company's strategy for growth?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9M_iU6_hbv0J"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "results = await asyncio.gather(*[\n",
        "    answer_question(index, question) for question in questions\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SZM_xg3b9Gb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(columns=[\"question\", \"answer\"], data=list(zip(questions, results)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnkK0NwIIM9q"
      },
      "source": [
        "### Improve accuracy with query rewriting / expansion\n",
        "\n",
        "We can also use the power on an LLM to rewrite or expand an input question.\n",
        "\n",
        "Example: https://github.com/langchain-ai/langchain/blob/master/templates/rewrite-retrieve-read/rewrite_retrieve_read/chain.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnWhfeiGYVrI"
      },
      "outputs": [],
      "source": [
        "# An example question that is a bit simplistic...\n",
        "await answer_question(index, \"How big is the company?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg55HqLFIRXJ"
      },
      "outputs": [],
      "source": [
        "async def rewrite_query(query: str):\n",
        "    \"\"\"Rewrite the user's original query\"\"\"\n",
        "\n",
        "    SYSTEM_PROMPT = \"\"\"Given the user's input question below, find a better or\n",
        "    more complete way to phrase this question in order to improve semantic search\n",
        "    engine retrieval quality over a set of SEC 10K PDF docs. Return the rephrased\n",
        "    question as a string in a JSON response under the key \"query\".\"\"\"\n",
        "\n",
        "    response = await openai.AsyncClient().chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        response_format={ \"type\": \"json_object\" },\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": f\"Original input question from user: {query}\"}\n",
        "        ],\n",
        "        temperature=0.1,\n",
        "        seed=42\n",
        "    )\n",
        "    # Response provided by LLM\n",
        "    return json.loads(response.choices[0].message.content)[\"query\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_ce8fC8KR50"
      },
      "outputs": [],
      "source": [
        "# Example Sinple Query Rewritten\n",
        "await rewrite_query(\"How big is the company?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ubNQrJOYL42"
      },
      "outputs": [],
      "source": [
        "async def answer_question(index: AsyncSearchIndex, query: str):\n",
        "    \"\"\"Answer the user's question\"\"\"\n",
        "\n",
        "    SYSTEM_PROMPT = \"\"\"You are a helpful financial analyst assistant that has access\n",
        "    to public financial 10k documents in order to answer users questions about company\n",
        "    performance, ethics, characteristics, and core information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Rewrite the query using an LLM\n",
        "    rewritten_query = await rewrite_query(query)\n",
        "    query_vector = hf.embed(rewritten_query)\n",
        "    # Fetch context from Redis using vector search\n",
        "    context = await retrieve_context(index, query_vector)\n",
        "    # Generate contextualized prompt and feed to OpenAI\n",
        "    response = await openai.AsyncClient().chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": promptify(rewritten_query, context)}\n",
        "        ],\n",
        "        temperature=0.1,\n",
        "        seed=42\n",
        "    )\n",
        "    # Response provided by LLM\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIO_jW6KYsMU"
      },
      "outputs": [],
      "source": [
        "# Now try again with query re-writing enabled\n",
        "await answer_question(index, \"How big is the company?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p97uL4g9T6LQ"
      },
      "source": [
        "### Improve performance and cut costs with LLM caching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7geEAsYST6LQ"
      },
      "outputs": [],
      "source": [
        "from redisvl.extensions.llmcache import SemanticCache\n",
        "\n",
        "llmcache = SemanticCache(\n",
        "    name=\"llmcache\",\n",
        "    vectorizer=hf,\n",
        "    redis_url=REDIS_URL,\n",
        "    ttl=120,\n",
        "    distance_threshold=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ALcQXAqT6LQ"
      },
      "outputs": [],
      "source": [
        "from functools import wraps\n",
        "\n",
        "# Create an LLM caching decorator\n",
        "def cache(func):\n",
        "    @wraps(func)\n",
        "    async def wrapper(index, query_text, *args, **kwargs):\n",
        "        query_vector = llmcache._vectorizer.embed(query_text)\n",
        "\n",
        "        # Check the cache with the vector\n",
        "        if result := llmcache.check(vector=query_vector):\n",
        "            return result[0]['response']\n",
        "\n",
        "        response = await func(index, query_text, query_vector=query_vector)\n",
        "        llmcache.store(query_text, response, query_vector)\n",
        "        return response\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "@cache\n",
        "async def answer_question(index: AsyncSearchIndex, query: str, **kwargs):\n",
        "    \"\"\"Answer the user's question\"\"\"\n",
        "\n",
        "    SYSTEM_PROMPT = \"\"\"You are a helpful financial analyst assistant that has access\n",
        "    to public financial 10k documents in order to answer users questions about company\n",
        "    performance, ethics, characteristics, and core information.\n",
        "    \"\"\"\n",
        "\n",
        "    context = await retrieve_context(index, kwargs[\"query_vector\"])\n",
        "    response = await openai.AsyncClient().chat.completions.create(\n",
        "        model=CHAT_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": promptify(query, context)}\n",
        "        ],\n",
        "        temperature=0.1,\n",
        "        seed=42\n",
        "    )\n",
        "    # Response provided by GPT-3.5\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXK_BXuhT6LQ"
      },
      "outputs": [],
      "source": [
        "query = \"What was Nike's revenue last year compared to this year??\"\n",
        "\n",
        "await answer_question(index, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mZpSpf9T6LQ"
      },
      "outputs": [],
      "source": [
        "query = \"What was Nike's total revenue in the last year compared to now??\"\n",
        "\n",
        "await answer_question(index, query)\n",
        "\n",
        "# notice no HTTP request to OpenAI since this question is \"close enough\" to the last one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaiF_ws7itsi"
      },
      "source": [
        "### Offload session history to Redis\n",
        "\n",
        "In order to preserve state in the conversation, it's imperitive to offload conversation history to a database that can handle high transaction throughput for writes/reads to limit system latency.\n",
        "\n",
        "We can store message history for a particular user session in a Redis List data type.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMOF7fJQdhgN"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "async def get_messages(index: AsyncSearchIndex, user_id: str) -> list:\n",
        "    \"\"\"Get all messages associated with a session\"\"\"\n",
        "    return [\n",
        "        json.loads(msg) for msg in await index.client.lrange(f\"messages:{user_id}\", 0, -1)\n",
        "    ]\n",
        "\n",
        "async def add_messages(index: AsyncSearchIndex, user_id: str, messages: list):\n",
        "    \"\"\"Add chat messages to a Redis List\"\"\"\n",
        "    return await index.client.rpush(\n",
        "        f\"messages:{user_id}\", *[json.dumps(msg) for msg in messages]\n",
        "    )\n",
        "\n",
        "async def clear_history(index: AsyncSearchIndex, user_id: str):\n",
        "    \"\"\"Clear session chat\"\"\"\n",
        "    await index.client.delete(f\"messages:{user_id}\")\n",
        "\n",
        "async def answer_question(index: AsyncSearchIndex, query: str):\n",
        "    \"\"\"Answer the user's question with historical context and caching baked-in\"\"\"\n",
        "\n",
        "    SYSTEM_PROMPT = \"\"\"You are a helpful financial analyst assistant that has access\n",
        "    to public financial 10k documents in order to answer users questions about company\n",
        "    performance, ethics, characteristics, and core information.\n",
        "    \"\"\"\n",
        "\n",
        "    query_vector = llmcache._vectorizer.embed(query)\n",
        "\n",
        "    # Check the cache with the vector\n",
        "    if result := llmcache.check(vector=query_vector):\n",
        "        answer = result[0]['response']\n",
        "    else:\n",
        "        context = await retrieve_context(index, query_vector)\n",
        "        session = await get_messages(index, \"tyler\")\n",
        "        messages = (\n",
        "            [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}] +\n",
        "            session +\n",
        "            [{\"role\": \"user\", \"content\": promptify(query, context)}]\n",
        "        )\n",
        "        # Response provided by GPT-4\n",
        "        response = await openai.AsyncClient().chat.completions.create(\n",
        "            model=CHAT_MODEL,\n",
        "            messages=messages,\n",
        "            temperature=0.1,\n",
        "            seed=42\n",
        "        )\n",
        "        answer = response.choices[0].message.content\n",
        "\n",
        "    # Add message history\n",
        "    await add_messages(index, \"tyler\", [\n",
        "        {\"role\": \"user\", \"content\": query},\n",
        "        {\"role\": \"assistant\", \"content\": answer}\n",
        "    ])\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z3RUvyxdhiz"
      },
      "outputs": [],
      "source": [
        "# Setup Session\n",
        "await clear_history(index, \"tyler\")\n",
        "\n",
        "# Simple Chat\n",
        "while True:\n",
        "    query = input()\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    answer = await answer_question(index, query)\n",
        "    print(answer, flush=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoPQMAShZ5Uy"
      },
      "outputs": [],
      "source": [
        "await index.client.lrange(\"messages:tyler\", 0, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l4uEgKzljes"
      },
      "source": [
        "## Your Next Steps\n",
        "\n",
        "While a good start, there is still more to do. **For example**:\n",
        "- we could utilize message history to generate an updated and contextualized query to use for retrieval and answer generation (with an LLM). Otherwise, there can be a disconnect between what a user is asking (in context) and what they are asking in isolation.\n",
        "- we could utilize an LLM to summarize conversation history to use as context instead of passing the whole slew of messages to the Chat endpoint.\n",
        "- we could utilize semantic properties of the message history (or summaries) in order to fetch only relevant conversation bits (vector search).\n",
        "- we could utilize a technique like HyDE ( a form of query rewriting ) to improve the retrieval quality from raw user input to source documents OR try to break down user questions into sub questions and fetch / join context based on the different searces.\n",
        "- we could incorporate semantic routing to take a broken down question and route to different data sources, indices, or query types (etc).\n",
        "- we could add semantic guardrails on the front end or back end of the conversation I/O to ensure we are within bounds of approved topics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wscs4Mvo1ujD"
      },
      "source": [
        "## Cleanup\n",
        "\n",
        "Clean up the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On6yNuQn1ujD"
      },
      "outputs": [],
      "source": [
        "await index.client.flushall()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU9qZsAn1ujD"
      },
      "source": [
        "Now that you have tried the easy-to-use RedisVL client, try your hand with LangChain -- the highest level of abstraction for using and integrating Redis as a vector database.\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/Redislabs-Solution-Architects/financial-vss/blob/main/langchain-03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}